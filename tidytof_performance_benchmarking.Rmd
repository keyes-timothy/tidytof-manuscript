---
output:
  pdf_document: default
  officedown::rdocx_document:
    mapstyles:
      Normal: First Paragraph
    reference_docx: reference.docx
    tables: 
      width: 1.0
  html_document:
    df_print: paged
bibliography: supplementary_references.bib
csl: biomed-central.csl
urlcolor: blue
---

```{r setup, include=FALSE}
# options
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE, 
  warning = FALSE,
  out.width = "100%"
)
```

```{r libraries_globals, message = FALSE, warning = FALSE}
# libraries
library(plyr)
library(immunoCluster)
library(Spectre)
library(cytofkit)
library(flowCore)
library(tidytof)
library(tidyverse)
library(microbenchmark)
library(patchwork)
filter <- dplyr::filter
count <- dplyr::count
summarize <- dplyr::summarize

# source benchmarking functions 
source(here::here("benchmarking", "benchmarking_functions.R"))
source(here::here("benchmarking", "benchmarking_script.R"))

# globals
largest_file_name <- TRUE
machine <- "galaxia"
num_repeats <- 1L
charizard <- 
  c(
    "base" = "#207394", 
    "flowcore" = "#eeb45a", 
    "cytofkit" = "#833118", 
    "immunocluster" = "#FFEEB0", 
    "spectre" = "#837b92", 
    "tidytof" = "#cd5241"
  )
engine_labels <- 
  c(
    "base" = "base", 
    "flowcore" = "flowCore", 
    "cytofkit" = "cytofkit",
    "immunocluster" = "immunoCluster", 
    "spectre" = "Spectre", 
    "tidytof" = "tidytof"
  )

high_level_apis <- c("cytofkit", "immunocluster", "spectre", "tidytof")
low_level_apis <- c("base", "flowcore", "tidytof")

# which benchmarking sections to run from scratch
set_up_csvs <- FALSE
run_io <- FALSE
run_downsample <- FALSE
run_preprocess <- FALSE
run_tsne <- FALSE
run_pca <- FALSE
run_umap <- FALSE
run_cluster <- FALSE
run_extract <- FALSE
run_memory <- FALSE
run_style <- TRUE

# should I print the large versions of each plot
print_large <- FALSE

# should messages be printed during benchmarking
verbose <- FALSE

# file paths 
if (machine == "galaxia") { 
  base_path <- file.path("~", "Box", "Tim", "Lab", "Data")
} else {
  base_path <- file.path("~", "Desktop", "data")
}
ddpr_path <-
  file.path(base_path, "DDPR_Data")

csv_path <- 
  file.path(base_path, "DDPR_Data_csv")

mini_path <- 
  file.path(base_path, "DDPR_Data_mini")

ddpr_files <- 
  dir(ddpr_path, full.names = TRUE)

spectre_path <- 
  file.path(base_path, "tidytof_benchmarking") |> 
  dir(full.names = TRUE) |> 
  purrr::pluck(1)

```

```{r run_all_benchmarking, warning = FALSE, message = TRUE}
# run all benchmarking from scratch 
benchmark_master(
  set_up_csvs = set_up_csvs,
  run_io = run_io,
  run_downsample = run_downsample,
  run_preprocess = run_preprocess,
  run_tsne = run_tsne,
  run_pca = run_pca,
  run_umap = run_umap,
  run_cluster = run_cluster,
  run_extract = run_extract,
  run_memory = run_memory,
  run_style = run_style,
  num_repeats = num_repeats,
  num_channels = 10,
  base_path = base_path, 
  verbose = verbose
)
filter <- dplyr::filter
count <- dplyr::count
summarize <- dplyr::summarize
```



```{r store_all_file_paths}
# find file paths corresponding to the highest number of repeats 
in_out_paths <- 
  tibble(
    process = 
      c("fcs", "csv", "downsampling", "preprocessing", "tsne", "pca", "umap",  "flowsom", "extraction"), 
    pattern = 
      paste0(
        c(
          "io_benchmarking_speed_", 
          "io_benchmarking_speed_csv_", 
          "downsample_benchmarking_speed_", 
          "preprocess_benchmarking_speed_", 
          "tsne_benchmarking_speed_", 
          "pca_benchmarking_speed_", 
          "umap_benchmarking_speed_", 
          "flowsom_benchmarking_speed_", 
          "extract_benchmarking_speed_"
        ), 
        "[:digit:]+"
      ), 
    largest_size = 
      map_int(
        .x = pattern, 
        .f = ~
          here::here("benchmarking") |> 
          dir() |> 
          str_extract(.x) |> 
          na.omit() |> 
          str_extract("[:digit:]+$") |> 
          as.integer() |> 
          unique() |> 
          sort(decreasing = TRUE) |> 
          pluck(1), 
      ), 
    file_name = 
      paste0(
        str_replace(
          string = pattern, pattern = "\\[.+\\]\\+", 
          as.character(largest_size)
        ),
        ".rds"
      )
  )

```


```{r declare_benchmark_plotting_functions}
plot_benchmarks <- 
  function(
    benchmark_data, 
    subtitle = "[Insert subtitle]", 
    api = c("all", "high", "low"), 
    dodge = 0
  ) {
    if (api == "all") { 
      benchmark_data <- 
        benchmark_data |> 
        select(num_cells, last_col()) |> 
        unnest(last_col()) 
    } else if (api == "high") { 
         benchmark_data <- 
           benchmark_data |> 
           select(num_cells, last_col()) |> 
           unnest(last_col()) |> 
           filter(engine %in% high_level_apis)
    } else if (api == "low") { 
      benchmark_data <- 
        benchmark_data |> 
        select(num_cells, last_col()) |> 
        unnest(last_col()) |> 
        filter(engine %in% low_level_apis) 
    }
    engines <- 
      benchmark_data |> 
      pull(engine) |> 
      unique()
    
    num_engines <- length(engines)
    
    my_palette <- charizard[which(names(charizard) %in% engines)]
    
    max_cells <- max(benchmark_data$num_cells)
    width <- max_cells * 0.075
    
    # make a plot to summarize 
    speed_plot <- 
      benchmark_data |> 
      group_by(num_cells, engine) |> 
      dplyr::summarize(
        mean_time = median(time), 
        max_time = quantile(time, 0.75), 
        min_time = quantile(time, 0.25)
      ) |> 
      dplyr::ungroup() |> 
      ggplot(
        aes(
          x = num_cells, 
          y = mean_time, 
          fill = engine, 
          color = engine, 
          ymin = min_time, 
          ymax = max_time
        )
      ) + 
      geom_errorbar(
        width = width, 
        #position = position_dodge(width = dodge)
      ) + 
      geom_line(aes(group = engine)) + 
      geom_point(
        shape = 21, 
        size = 2, 
        color = "black", 
        position = position_dodge(width = dodge)
      ) + 
      scale_x_continuous(labels = scales::label_scientific(digits = 1)) + 
      scale_fill_manual(values = my_palette, labels = engine_labels) + 
      scale_color_manual(values = my_palette, labels = engine_labels) +
      theme_bw() + 
      labs(
        subtitle = subtitle,
        x = "Number of cells", 
        y = "Time (seconds)", 
        fill = NULL, 
        color = NULL
      )
    
    return(speed_plot)
    
  }

plot_benchmarks_split <- function(
  benchmark_data, 
  subtitle =  "[Insert subtitle]", 
  dodge = FALSE
) { 
  full_plot <- 
    benchmark_data |> 
    plot_benchmarks(subtitle = subtitle, api = "all", dodge = dodge)
  
  high_plot <- 
    benchmark_data |> 
    plot_benchmarks(subtitle = subtitle, api = "high", dodge = dodge) 
  
  low_plot <- 
    benchmark_data |> 
    plot_benchmarks(subtitle = subtitle, api = "low", dodge = dodge)
  
  result <- 
    list(
      full_plot = full_plot, 
      high_plot = high_plot, 
      low_plot = low_plot
    )
  return(result)
}

save_bench_plots <- 
  function(benchmarking_plots, file_name) {
    
    full_plot <- benchmarking_plots$full_plot
    high_plot <- benchmarking_plots$high_plot
    low_plot <- benchmarking_plots$low_plot
    
    low_name <- str_c("low_level_", file_name)
    high_name <- str_c("high_level_", file_name)
    
    full_plot |> 
      ggsave(
        filename = 
          here::here(
            "benchmarking", 
            str_replace(file_name, ".rds", ".pdf")
          ), 
        device = "pdf", 
        width = 5, 
        height = 3
      )
    
    low_plot |> 
      ggsave(
        filename = 
          here::here(
            "benchmarking", 
            str_replace(low_name, ".rds", ".pdf")
          ), 
        device = "pdf", 
        width = 5, 
        height = 3
      )
    
    high_plot |> 
      ggsave(
        filename = 
          here::here(
            "benchmarking", 
            str_replace(high_name, ".rds", ".pdf")
          ), 
        device = "pdf", 
        width = 5, 
        height = 3
      )
  }
```


```{r io_file_names}
if (largest_file_name) { 
# file paths
file_name <- 
  in_out_paths |> 
  filter(process == "fcs") |> 
  pull(file_name)

file_name_csv <- 
  in_out_paths |> 
  filter(process == "csv") |> 
  pull(file_name)
} else {
  file_name <- 
    in_out_paths |> 
    filter(process == "fcs") |> 
    pull(pattern) |> 
    str_replace("\\[\\:digit\\:\\]\\+", as.character(num_repeats)) |> 
    str_c(., ".rds")
  
  file_name_csv <- 
    in_out_paths |> 
    filter(process == "csv") |> 
    pull(pattern) |> 
    str_replace("\\[\\:digit\\:\\]\\+", as.character(num_repeats)) |> 
    str_c(., ".rds")
}
```






```{r visualize_io_result, message = FALSE}
io_benchmarking <- 
  read_rds(file = here::here("benchmarking", file_name))

io_speed_plots <- 
  io_benchmarking |> 
  plot_benchmarks_split(subtitle = "FCS reader", dodge = 1000)

io_speed_plot <- io_speed_plots$full_plot
io_high_plot <- io_speed_plots$high_plot
io_low_plot <- io_speed_plots$low_plot

if (print_large) {
  print(io_speed_plots)
}

io_speed_plots |> 
  save_bench_plots(file_name = file_name)

```


```{r visualize_io_result_csv, message = FALSE}
io_benchmarking_csv <- 
  read_rds(file = here::here("benchmarking", file_name_csv))

io_speed_plots_csv <- 
  io_benchmarking_csv |> 
  plot_benchmarks_split(subtitle = "CSV reader")

io_speed_plot_csv <- io_speed_plots_csv$full_plot
io_high_plot_csv <- io_speed_plots_csv$high_plot
io_low_plot_csv <- io_speed_plots_csv$low_plot

if (print_large) {
  print(io_speed_plots_csv)
}

io_speed_plots_csv |> 
  save_bench_plots(file_name = file_name_csv)
```


```{r downsample_file_names}
if (largest_file_name) { 
  file_name <- 
    in_out_paths |> 
    filter(process == "downsampling") |> 
    pull(file_name)
  
} else {
  file_name <- 
    in_out_paths |> 
    filter(process == "downsampling") |> 
    pull(pattern) |> 
    str_replace("\\[\\:digit\\:\\]\\+", as.character(num_repeats)) |> 
    str_c(., ".rds")
}

```


```{r visualize_downsampling_result, message = FALSE}
downsample_benchmarking <- 
  read_rds(file = here::here("benchmarking", file_name))

downsample_speed_plots <- 
  downsample_benchmarking |> 
  plot_benchmarks_split(subtitle = "Downsampling", dodge = 5000)

downsample_speed_plots |> 
  save_bench_plots(file_name = file_name)

if (print_large) {
  print(downsample_speed_plots)
}

```


```{r preprocess_file_name}
file_name <- 
  in_out_paths |> 
  filter(process == "preprocessing") |> 
  pull(file_name)
```



```{r visualize_benchmarking_result, message = FALSE}
preprocess_benchmarking <- 
  read_rds(file = here::here("benchmarking", file_name))

preprocess_speed_plots <- 
  preprocess_benchmarking |> 
  plot_benchmarks_split(subtitle = "Preprocessing", dodge = 7500)

preprocess_speed_plots |> 
  save_bench_plots(file_name = file_name)


if (print_large) {
  print(preprocess_speed_plots)
}

```



```{r dimensionality_reduction_file_names}
file_name_tsne <- 
  in_out_paths |> 
  filter(process == "tsne") |> 
  pull(file_name)

file_name_pca <- 
  in_out_paths |> 
  filter(process == "pca") |> 
  pull(file_name)

file_name_umap <- 
  in_out_paths |> 
  filter(process == "umap") |> 
  pull(file_name)
```



```{r visualize_tsne_result, message = FALSE}
tsne_benchmarking <- 
  read_rds(file = here::here("benchmarking", file_name_tsne))

tsne_speed_plots <- 
  tsne_benchmarking |> 
  plot_benchmarks_split(subtitle = "tSNE", dodge = 10)

tsne_speed_plot <- tsne_speed_plots$full_plot

if (print_large) {
  print(tsne_speed_plots)
}

tsne_speed_plots |> 
  save_bench_plots(file_name = file_name_tsne)


```



```{r visualize_pca_result, message = FALSE}
pca_benchmarking <- 
  read_rds(file = here::here("benchmarking", file_name_pca))

pca_speed_plots <- 
  pca_benchmarking |> 
  plot_benchmarks_split(subtitle = "PCA", dodge = 2000)

if (print_large) {
  pca_speed_plots
}

pca_speed_plots |> 
  save_bench_plots(file_name = file_name_pca)

```


```{r visualize_umap_result, message = FALSE}
umap_benchmarking <- 
  read_rds(file = here::here("benchmarking", file_name_umap))

umap_speed_plots <- 
  umap_benchmarking |> 
  plot_benchmarks_split(subtitle = "UMAP", dodge = 40)

umap_speed_plots |> 
  save_bench_plots(file_name = file_name_umap)

if (print_large) {
  umap_speed_plots
}

```



```{r flowsom_file_names}
file_name <- 
  in_out_paths |> 
  filter(process == "flowsom") |> 
  pull(file_name)
```


```{r visualize_flowsom_result, message = FALSE}
cluster_benchmarking <-
  read_rds(file = here::here("benchmarking", file_name))

flowsom_speed_plots <- 
  cluster_benchmarking |> 
  plot_benchmarks_split(subtitle = "FlowSOM", dodge = 1000)

flowsom_speed_plots |> 
  save_bench_plots(file_name = file_name)

if (print_large) {
  flowsom_speed_plots
}

```


```{r extraction_file_name}
file_name <- 
  in_out_paths |> 
  filter(process == "extraction") |> 
  pull(file_name)
```


```{r visualize_extraction_result, message = FALSE}
extract_benchmarking <- 
  read_rds(file = here::here("benchmarking", file_name))

base_extract <-
  read_rds(file = here::here("benchmarking", "base_extract.rds"))


extract_benchmarking <-
  extract_benchmarking |>
  mutate(
    base_extract_speed = base_extract$extract_benchmarks,
    extract_benchmarks =
      map2(
        .x = extract_benchmarks,
        .y = base_extract_speed,
        .f = bind_rows
      )
  ) |>
  select(-base_extract_speed)

extract_speed_plots <- 
  extract_benchmarking |> 
  plot_benchmarks_split(subtitle = "Feature extraction", dodge = 0)

extract_speed_plots |> 
  save_bench_plots(file_name = file_name)

if (print_large) {
  print(extract_speed_plots)
}

```


```{r impute_tsne}
tsne_data <- 
  tsne_benchmarking |> 
  unnest(cols = tsne_benchmarks) |> 
  group_by(num_cells, engine) |> 
  summarize(time = mean(time)) |> 
  ungroup() |> 
  mutate(engine = as.factor(engine))

tsne_model <- 
  lm(time ~ num_cells * engine, data = tsne_data)

tsne_imputation_data <- 
  expand_grid(
    num_cells = io_benchmarking$num_cells, 
    engine = unique(tsne_data$engine)
  ) |> 
  mutate(engine = as.factor(engine))



tsne_imputations <- 
  predict(tsne_model, newdata = tsne_imputation_data)

tsne_imputation_data <- 
  tsne_imputation_data |> 
  mutate(time = tsne_imputations)


```


```{r impute_pca}
pca_data <- 
  pca_benchmarking |> 
  unnest(cols = pca_benchmarks) |> 
  group_by(num_cells, engine) |> 
  summarize(time = mean(time)) |> 
  ungroup() |> 
  mutate(engine = as.factor(engine))

pca_model <- 
  lm(time ~ num_cells * engine, data = pca_data)

pca_imputation_data <- 
  expand_grid(
    num_cells = io_benchmarking$num_cells, 
    engine = unique(pca_data$engine)
  ) |> 
  mutate(engine = as.factor(engine))

pca_imputations <- 
  predict(pca_model, newdata = pca_imputation_data)

pca_imputation_data <- 
  pca_imputation_data |> 
  mutate(time = pca_imputations)

```


```{r impute_umap}
umap_data <- 
  umap_benchmarking |> 
  unnest(cols = umap_benchmarks) |> 
  group_by(num_cells, engine) |> 
  summarize(time = mean(time)) |> 
  ungroup() |> 
  mutate(engine = as.factor(engine))

umap_model <- 
  lm(time ~ num_cells * engine, data = umap_data)

umap_imputation_data <- 
  expand_grid(
    num_cells = io_benchmarking$num_cells, 
    engine = unique(umap_data$engine)
  ) |> 
  mutate(engine = as.factor(engine))

umap_imputations <- 
  predict(umap_model, newdata = umap_imputation_data)

umap_imputation_data <- 
  umap_imputation_data |> 
  mutate(time = umap_imputations)

```

```{r}
dr_tibble <- 
  bind_rows(
    mutate(tsne_imputation_data, workflow = "tsne"), 
    mutate(pca_imputation_data, workflow = "pca"), 
    mutate(umap_imputation_data, workflow = "umap")
  )

```



```{r visualize_total_time, message = FALSE}
benchmark_names <- 
  ls()[str_detect(ls(), "_benchmarking")] |> 
  discard(.p = ~str_detect(., "tsne|umap|pca"))

all_benchmarks <- 
  mget(benchmark_names)

all_benchmarks <- 
  map2(
    .x = all_benchmarks, 
    .y = names(all_benchmarks), 
    .f = ~ 
      dplyr::rename(.x, benchmarks = last_col()) |> 
      dplyr::mutate(workflow = .y)
  )

all_benchmarks_df2 <- 
  all_benchmarks |> 
  bind_rows() |> 
  filter(!str_detect(workflow, "pca|tsne|umap")) |> 
  unnest(cols = benchmarks) |> 
  bind_rows(dr_tibble) |>
  # find the average of each workflow for each engine across all replicates
  group_by(num_cells, engine, workflow) |> 
  summarize(time = mean(time)) |> 
  # find the speed per cell for each workflow for each engine
  ungroup() |> 
  # mutate(
  #   engine = 
  #     case_when(
  #       engine == "tidytof" ~ "tidytof", 
  #       engine %in% low_level_apis ~ "low-level APIs", 
  #       engine %in% high_level_apis ~ "high-level APIs",
  #       TRUE ~ "NA"
  #     )
  # ) |> 
  # find the average of times for all workflows across all APIs
  group_by(num_cells, engine, workflow) |> 
  summarize(time = mean(time)) |> 
  ungroup()

all <- 
  tidyr::expand(all_benchmarks_df2, engine, workflow, num_cells)

base_benchmarks <- 
  all_benchmarks_df2 |> 
  filter(engine == "base") |> 
  mutate(impute_time = time) |> 
  select(num_cells, workflow, impute_time)

all_benchmarks_df2 <- 
  all_benchmarks_df2 |> 
  right_join(all) |> 
  left_join(base_benchmarks, by = c("num_cells", "workflow")) |> 
  mutate(time = if_else(is.na(time), impute_time, time)) |> 
  filter(workflow != "tsne") |> 
  filter(workflow != "pca") |> 
  filter(workflow != "umap") |> 
  group_by(num_cells, engine) |> 
  summarize(time = sum(time))

total_time_plot <- 
  all_benchmarks_df2 |> 
  #filter(engine == "base") |> 
  ggplot(aes(x = num_cells, y = time, fill = engine, color = engine)) + 
  geom_line() + 
  geom_point(
    shape = 21, 
    size = 3, 
    color = "black", 
    position = position_dodge(width = 10000)
  ) + 
  # ggthemes::scale_fill_tableau() +  
  # ggthemes::scale_color_tableau() + 
  scale_color_manual(values = charizard, labels = engine_labels) + 
  scale_fill_manual(values = charizard, labels = engine_labels) + 
  scale_x_continuous(
    breaks = scales::breaks_pretty(n = 5), 
    labels = scales::label_scientific()
  ) + 
  theme_bw() +
  labs(
    x = "Number of cells", 
    y = "Time (seconds)", 
    subtitle = "Total time", 
    color = NULL, 
    fill = NULL
  )

if (print_large) { 
  print(total_time_plot)
}
  
```



```{r visualize_lines_of_code}
code_tibble <- 
  read_rds(file = here::here("benchmarking", "code_style.rds")) #|> 
  # mutate(engine = factor(engine, levels = names(engine_labels)))

# visualize lines of code
engines <- 
  code_tibble |> 
  pull(engine) |> 
  unique()

plot_lines_raw <- function(
  code_tibble, 
  engines = c("base", "tidytof", "flowcore", "immunocluster", "spectre", "cytofkit")
) {
  my_palette <- charizard[which(names(charizard) %in% engines)]
  
  lines_plot_raw <- 
    code_tibble |> 
    filter(measure == "lines", engine %in% engines) |> 
    ggplot(aes(x = analysis, y = number, fill = engine)) + 
    geom_line(aes(group = engine, color = engine)) + 
    geom_point(shape = 21, size = 3, position = position_dodge(width = 0.05)) + 
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
    scale_x_discrete(
      labels = 
        function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
    ) + 
    scale_fill_manual(values = my_palette, labels = engine_labels) + 
    scale_color_manual(values = my_palette, labels = engine_labels) + 
    labs(
      x = NULL, 
      y = "Lines of code", 
      fill = NULL, 
      color = NULL
    )
  
  return(lines_plot_raw)
}

lines_plot_raw <- plot_lines_raw(code_tibble)
lines_plot_low <- plot_lines_raw(code_tibble, low_level_apis)
lines_plot_high <- plot_lines_raw(code_tibble, high_level_apis)

if (print_large) { 
  print(lines_plot_raw)
  print(lines_plot_low)
  print(lines_plot_high)
}


```


```{r save_raw_lines}
lines_plot_raw |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_plot_raw.pdf"), 
    device = "pdf", 
    width = 5, 
    height = 3.5
  )
```


```{r analyze_relative_lines}
make_lines_plot <- function(code_tibble, reference_engine = "base", black_text = FALSE) { 
  plot_tibble_engine <- 
    code_tibble |> 
    filter(engine %in% c("tidytof", reference_engine)) |> 
    filter(measure == "lines") |> 
    group_by(analysis) |> 
    mutate(
      lines = number, 
      number = number / max(number), 
      reduction = paste0(" (-", scales::label_percent()(1 - number), ")"), 
    ) |> 
    ungroup()
  
  plot_tibble_engine <- 
    plot_tibble_engine |> 
    drop_na()
  
  lines_plot_engine <- 
    plot_tibble_engine |> 
    ggplot(aes(x = analysis, y = number, fill = engine)) + 
    geom_segment(
      mapping = aes(x = analysis, y = 1, xend = analysis, yend = number + 0.04),
      data = filter(plot_tibble_engine, engine == "tidytof"), 
      size = 1.5, 
      color = "gray60", 
      arrow = arrow(length = unit(0.1, "inches"), type = "closed"), 
      alpha = 0.9, 
      linejoin = "mitre", 
      lineend = "butt"
    ) + 
    # horizontal reference line
    geom_hline(yintercept = 1, size = 1, color = "black", linetype = "dotted") + 
    # points for each engine
    geom_point(shape = 21, size = 4) + 
    # percentage reduction text
    geom_text(
      mapping = aes(label = reduction), 
      data = filter(plot_tibble_engine, engine == "tidytof"), 
      nudge_x = 0, 
      nudge_y = -0.12, 
      size = 3
    ) + 
    # tidytof lines of text (bottom text)
    geom_text(
      aes(
        color = engine, 
        label = paste0(lines, if_else(lines == 1, " line", " lines"))
      ), 
      data = filter(plot_tibble_engine, engine == "tidytof"), 
      nudge_y = -0.06, 
      show.legend = FALSE, 
      size = 3
    )
  
  
  if (black_text) {
    # black top text
    lines_plot_engine <- 
      lines_plot_engine + 
      geom_text(
        aes(label = paste0(lines, if_else(lines == 1, " line", " lines"))), 
        data = filter(plot_tibble_engine, engine == reference_engine), 
        color = "black", 
        nudge_y = 0.06, 
        show.legend = FALSE, 
        size = 3
      )
    
  } else { 
    # colored top text
    lines_plot_engine <- 
      lines_plot_engine +
      geom_text(
        aes(color = engine, label = paste0(lines, " lines")),
        data = filter(plot_tibble_engine, engine == reference_engine),
        nudge_y = 0.06,
        show.legend = FALSE,
        size = 3
      )
  }
  
  lines_plot_engine <- 
    lines_plot_engine + 
    scale_x_discrete(
      labels = 
        function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
    ) + 
    scale_y_continuous(
      breaks = c(0, 0.25, 0.5, 0.75, 1), 
      labels = scales::label_percent()
    ) + 
    scale_fill_manual(values = charizard, labels = engine_labels) + 
    scale_color_manual(values = charizard, labels = engine_labels) +
    theme_bw() + 
    theme(
      axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)
    ) + 
    labs(
      x = NULL, 
      y = str_glue(
        "Lines of code (% of {my_engine})", 
        my_engine = 
          case_when(
            reference_engine == "base"          ~ "base R", 
            reference_engine == "flowcore"      ~ "flowCore", 
            reference_engine == "immunocluster" ~ "immunoCluster", 
            reference_engine == "spectre"       ~ "Spectre", 
            reference_engine == "cytofkit"      ~ "cytofkit", 
            TRUE                                ~ "tidytof"
          )
      ), 
      fill = NULL, 
      color = NULL
    )
  
  return(lines_plot_engine)
}
```

```{r}
lines_plots <- 
  unique(code_tibble$engine) |> 
  discard(.p = ~ .x == "tidytof") |> 
  map(.f = ~make_lines_plot(code_tibble, .x))
names(lines_plots) <- 
  unique(code_tibble$engine) |> 
  discard(.p = ~ .x == "tidytof")

lines_plots$immunocluster <- 
  make_lines_plot(code_tibble, "immunocluster", black_text = TRUE)


if (print_large) {
  print(lines_plots)
}

```



```{r save_relative_lines_plots}
lines_plots$base |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_plot_base.pdf"), 
    device = "pdf", 
    width = 6, 
    height = 4
  )

lines_plots$flowcore |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_plot_flowcore.pdf"), 
    device = "pdf", 
    width = 6, 
    height = 4
  )

lines_plots$cytofkit |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_plot_cytofkit.pdf"), 
    device = "pdf", 
    width = 6, 
    height = 4
  )

lines_plots$spectre |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_plot_spectre.pdf"), 
    device = "pdf", 
    width = 6, 
    height = 4
  )

lines_plots$immunocluster |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_plot_immunocluster.pdf"), 
    device = "pdf", 
    width = 6, 
    height = 4
  )


```

```{r analyze_raw_variables, message = FALSE, warning = FALSE}
variables_plot_raw <- 
  code_tibble |> 
  filter(measure == "variables") |> 
  ggplot(aes(x = analysis, y = number, fill = engine)) + 
  geom_line(aes(group = engine, color = engine)) + 
  geom_point(shape = 21, size = 3) + 
  scale_x_discrete(
    labels = 
      function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
  ) + 
  scale_fill_manual(values = charizard, labels = engine_labels) + 
  scale_color_manual(values = charizard, labels = engine_labels) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(
    x = NULL, 
    y = "Intermediate variable assignments", 
    fill = NULL, 
    color = NULL
  )

if (print_large) {
  variables_plot_raw
}
```

```{r save_raw_variables}
variables_plot_raw |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_plot_raw.pdf"), 
    device = "pdf", 
    width = 4.5, 
    height = 3.5
  )
```



```{r analyze_relative_variables_base}
make_variables_plot <- function(code_tibble, reference_engine, black_text = FALSE) {
  plot_tibble_engine <- 
    code_tibble |> 
    dplyr::filter(engine %in% c("tidytof", reference_engine)) |> 
    dplyr::filter(measure == "variables") |> 
    group_by(analysis) |> 
    mutate(
      lines = number, 
      number = number / max(number), 
      reduction = paste0(" (-", scales::label_percent()(1 - number), ")"), 
    ) |> 
    ungroup()
  
  plot_tibble_engine <- 
    plot_tibble_engine |> 
    drop_na()
  
  variables_plot_engine <- 
    plot_tibble_engine |> 
    ggplot(aes(x = analysis, y = number, fill = engine)) + 
    geom_segment(
      mapping = aes(x = analysis, y = 1, xend = analysis, yend = number + 0.04),
      data = dplyr::filter(plot_tibble_engine, engine == "tidytof"), 
      size = 1.5, 
      color = "gray60", 
      arrow = arrow(length = unit(0.1, "inches"), type = "closed"), 
      alpha = 0.9, 
      linejoin = "mitre", 
      lineend = "butt"
    ) + 
    geom_hline(yintercept = 1, size = 1, color = "black", linetype = "dotted") + 
    geom_point(shape = 21, size = 4) + 
    geom_text(
      mapping = aes(label = reduction), 
      data = dplyr::filter(plot_tibble_engine, engine == "tidytof"), 
      nudge_x = 0, 
      nudge_y = -0.12, 
      size = 3
    ) 
  
    if (black_text) {
    # black top text
    variables_plot_engine <- 
      variables_plot_engine + 
      geom_text(
        aes(label = paste0(lines, if_else(lines == 1, " var", " vars"))), 
        data = filter(plot_tibble_engine, engine == reference_engine), 
        color = "black", 
        nudge_y = 0.06, 
        show.legend = FALSE, 
        size = 3
      )
    
  } else { 
    # colored top text
    variables_plot_engine <- 
      variables_plot_engine +
      geom_text(
        aes(color = engine, label = paste0(lines, " vars")),
        data = filter(plot_tibble_engine, engine == reference_engine),
        nudge_y = 0.06,
        show.legend = FALSE,
        size = 3
      )
  }
  
  variables_plot_engine <- 
    variables_plot_engine + 
    geom_text(
      aes(
        color = engine, 
        label = paste0(lines, if_else(lines == 1, " var", " vars"))), 
      data = filter(plot_tibble_engine, engine == "tidytof"), 
      nudge_y = -0.06, 
      show.legend = FALSE, 
      size = 3
    ) + 
    scale_x_discrete(
      labels = 
        function(x) if_else(x %in% c("pca", "tsne", "umap"), str_to_upper(x), str_to_title(x))
    ) + 
    scale_y_continuous(
      breaks = c(0, 0.25, 0.5, 0.75, 1), 
      labels = scales::label_percent()
    ) +     
    scale_fill_manual(values = charizard, labels = engine_labels) + 
    scale_color_manual(values = charizard, labels = engine_labels) +
    theme_bw() + 
    theme(
      axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3)
    ) + 
    labs(
      x = NULL, 
      y = str_glue(
        "Assigned variables (% of {my_engine})",
        my_engine = 
          case_when(
            reference_engine == "base"          ~ "base R", 
            reference_engine == "flowcore"      ~ "flowCore", 
            reference_engine == "immunocluster" ~ "immunoCluster", 
            reference_engine == "spectre"       ~ "Spectre", 
            reference_engine == "cytofkit"      ~ "cytofkit", 
            TRUE                                ~ "tidytof"
          )
      ), 
      fill = NULL, 
      color = NULL
    )
  return(variables_plot_engine)
}

variables_plots <- 
  unique(code_tibble$engine) |> 
  discard(.p = ~ .x == "tidytof") |> 
  map(.f = ~ make_variables_plot(code_tibble, .x))

names(variables_plots) <- 
  unique(code_tibble$engine) |> 
  discard(.p = ~ .x == "tidytof")

variables_plots$immunocluster <- 
  make_variables_plot(code_tibble, "immunocluster", black_text = TRUE)

if (print_large) {
  variables_plots
}

```



```{r save_relative_variables_plots}
variables_plots$base |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_plot_base.pdf"), 
    device = "pdf", 
    width = 8, 
    height = 5
  )

variables_plots$flowcore |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_plot_flowcore.pdf"), 
    device = "pdf", 
    width = 6.5, 
    height = 5
  )

variables_plots$cytofkit |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_plot_cytofkit.pdf"), 
    device = "pdf", 
    width = 6.5, 
    height = 5
  )

variables_plots$spectre |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_plot_spectre.pdf"), 
    device = "pdf", 
    width = 6.5, 
    height = 5
  )

variables_plots$immunocluster |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_plot_immunocluster.pdf"), 
    device = "pdf", 
    width = 6.5, 
    height = 5
  )
```





```{r visualize_memory_results}
average_mem_ratio <- 
  read_rds(file = here::here("benchmarking", "memory.rds")) |> 
  mutate(mem_ratio = tidytof_memory / flowcore_memory) |> 
  summarize(mem_ratio = mean(mem_ratio)) |> 
  pull(mem_ratio)

memory_tibble <- 
  read_rds(file = here::here("benchmarking", "memory.rds")) |> 
  pivot_longer(
    cols = contains("_memory"), 
    names_to = "engine", 
    values_to = "memory"
  ) |> 
  mutate(engine = str_remove(engine, "_memory"))

engines <- 
  memory_tibble |> 
  pull(engine) |> 
  unique()

my_palette <- charizard[which(names(charizard) %in% engines)]

memory_plot <- 
  memory_tibble |> 
  filter(engine != "tidytof") |> 
  ggplot(aes(x = num_cells, y = memory, fill = engine, color = engine)) + 
  geom_line() +
  geom_point(shape = 21, size = 2, color = "black", position = position_dodge(width = 2000)) + 
  geom_point(
    shape = 21, 
    size = 2, 
    color = "black", 
    data = filter(memory_tibble, engine == "tidytof")
  ) + 
  scale_x_continuous(labels = scales::label_scientific(digits = 1)) + 
  scale_fill_manual(values = my_palette, labels = engine_labels) + 
  scale_color_manual(values = my_palette, labels = engine_labels) +
  theme_bw() + 
  labs(
    subtitle = "Dataset memory requirements",
    x = "Number of cells", 
    y = "Memory (MB)", 
    fill = NULL, 
    color = NULL
  )

memory_plot |> 
  ggsave(
    filename = 
      here::here("benchmarking", "memory_plot.png"), 
    device = "png", 
    width = 5, 
    height = 3
  )

if (print_large) { 
  print(memory_plot)
}


memory_plot |> 
  ggsave(
    filename = 
      here::here("benchmarking", "memory_plot.pdf"), 
    device = "pdf", 
    width = 5, 
    height = 3
  )
  
```

## Methods

```{r}
ddpr_data_info <-
  read_rds(
    file = here::here("benchmarking", "ddpr_data_info.rds")
  )

largest_cohort_names <-
  ddpr_data_info |>
  filter(num_files == 288L) |>
  pull(file_names) |>
  pluck(1)

all_file_names <-
  largest_cohort_names[1:20]

smallest_num_cells <- 
  memory_tibble |> 
  pull(num_cells) |> 
  min()

total_num_cells <- 
  memory_tibble |> 
  pull(num_cells) |> 
  max()

```


To benchmark {tidytof}'s performance against existing tools for high-dimensional cytometry data analysis, we compared {tidytof} functions to their equivalent workflows (where they existed) using two low-level APIs (base R @baseR and {flowCore} @flowcore) and three high-level APIs ({cytofkit} @cytofkit, {immunoCluster} @immunocluster, and {Spectre} @spectre) with similar capabilities. Workflows were chosen to represent common CyTOF data processings tasks including reading input files, data preprocessing, downsampling, clustering, dimensionality reduction, and sample-level feature extraction (i.e. calculating summary statistics). 

Workflows were compared both on the basis of their computational speed and on the basis of their coding burden (i.e. the amount of code needed for each workflow). Specifically, comparisons included one metric for computational speed (total elapsed time for each workflow) and two metrics for coding burden associated with human error when conducting computational analyses (the number of lines of code and the number of intermediate variable assignments needed for the workflow). In addition, {tidytof}'s memory requirements were benchmarked against those of the {flowCore} package's `flowSet` objects - a commonly-used data structure for the analysis of high-dimensional cytometry data - as well as against {cytofkit} `data.frame`s, {immunoCluster} `SingleCellExperiment`s @sce, and {Spectre} `data.table`s @data.table. 

All benchmarking was performed on a Linux machine running Ubuntu 20.04 with an AMD Threadripper Pro 3995WX processor (64 cores; 2.70 GHz; 256 MB cache) and 256 GB of RAM. All functions used to perform speed, coding burden, and memory benchmarking are available [here](https://github.com/keyes-timothy/tidytof/blob/main/manuscript/benchmarking/benchmarking_functions.R).


### Benchmarking datasets

The data used for benchmarking were drawn from Good et al.'s publicly available CyTOF dataset published in 2018 @ddpr. Specifically, 20 files acquired from 2 healthy patients were chosen from the full dataset, totaling `r total_num_cells` cells. The following files were used: 

```{r, results = "asis"}
brdr <- officer::fp_border(color = "black", width = 0.5)

tibble(`File name` = all_file_names) |> 
  flextable::flextable() |> 
  flextable::bold(i = 1, part = "header") |> 
  flextable::fontsize(part = "all", size = 10) |> 
  flextable::hline_top(part = "header", border = brdr) |> 
  flextable::hline_bottom(part = "header", border = brdr) |> 
  flextable::hline_bottom(part = "body", border = brdr) |> 
  flextable::autofit() |> 
  flextable::flextable_to_rmd(ft.arraystretch = 1.1)
```

### Speed benchmarking

For speed benchmarking, each workflow was run on 20 nested datasets derived from the files listed above such that each dataset *i* was comprised of the first *i* files in the list. Thus, the smallest dataset included only 1 file (and `r smallest_num_cells` cells) while the largest dataset included all 20 files (and `r total_num_cells` cells). However, due to the computationally intensive nature of t-stochastic neighborhood embedding (tSNE) and uniform manifold approximation and projection (UMAP), speed benchmarking was performed using 10 subsampled versions of the full dataset (all 20 files) with a smaller number of cells drawn at random: 1K, 2K, 3K, 4K, 5K, 6K, 7K, 8K, 9K, and 10K.

Using these datasets, the `microbenchmark` package @microbenchmark was used to time each workflow independently on each dataset 10 times. The median and interquartile range of each workflow's runtime could then be computed for {tidytof} and each alternative tool. For dimensionality reduction and clustering analyses, only cluster of differentiation ("CD") markers were used for analysis. For downsampling, 200 cells were subsampled from each input FCS or CSV file. Otherwise, {tidytof}'s default behavior was used across all benchmarking functions.

### Memory benchmarking

For memory benchmarking, each nested dataset described above was read into each of the following: 

* A `tof_tbl` object using {tidytof}'s `tof_read_data()` function
* A `flowSet` object using {flowCore}'s `read.flowSet()` function 
* A `data.frame` using {cytofkit}'s `cytof_exprsMerge()` function 
* A `SingleCellExperiment` object using {immunoCluster}'s `processFCS()` function 
* A `data.table` using {Spectre}'s `read.files()` function. 

The system memory used to store each object was then calculated using the `obj_size()` function from the {lobstr} package @lobstr. 

### Coding burden benchmarking

For coding burden benchmarking, the source code of the functions containing each workflow was subjected to text analysis. To control for differences in code style and practices between workflows, the {styler} package @styler was used to standardize all benchmarking code to adhere to the [tidyverse R code style guide](https://style.tidyverse.org/). The functions used to count the number of lines of code and the number of variable assignments in each workflow are provided here:


```{r, echo = TRUE, include = TRUE, eval = FALSE}
# count the lines of code for a given workflow
get_lines <- function(function_object) { 
  if (is.na(function_object)) {
    return(NA)
  } else {
    lines <- capture.output(print(function_object))
    # substract 2 lines to omit the function definition and closing 
    # bracket for each workflow
    return(length(lines) - 2L)
  }
}

# count the number of assigned variables for a given workflow
get_assignments <- function(function_object) {
  if (is.na(function_object)) {
    return(NA)
  } else {
    lines <- capture.output(print(function_object))
    assignments <- max(sum(str_count(lines, pattern = "<-")), 1L)
    return(assignments)
  }
}
```

For both functions above, the input is a function (`function_object`) used to define a {tidytof}, base R, {flowCore}, {cytofkit}, {immunoCluster}, or {Spectre} workflow, and the output is an integer representing the number of lines of code or the number of variable assignments, respectively, contained in that workflow. The functions used to define each workflow are available on GitHub [here](https://github.com/keyes-timothy/tidytof/blob/main/manuscript/benchmarking/benchmarking_functions.R). 

## Results 

### Speed benchmarking

Across a wide variety of workflows including FCS and CSV file reading, single-cell data transformation (preprocessing), downsampling, FlowSOM clustering, sample-level feature extraction, tSNE embedding, and UMAP embedding, {tidytof}'s time efficiency rivals or improves upon alternative approaches (**Supplementary Figure 1**). In the case of PCA, {tidytof} incurs a slight performance decrease relative to alternative tools because it uses the `{embed}` package @embed to store a dataset's PCA embedding as a `recipe` object (from the {recipes} package) @recipes - which, while costing a small amount of computational overhead, allows users to more easily apply the same embedding to new data points. While {Spectre}'s use of `data.table` makes it slightly faster than {tidytof} for simple arithmetic tasks like data preprocessing (**Supplementary Figure 1C**) and  downsampling (**Supplementary Figure 1D**), {tidytof} requires substantially less cumulative time than competing methods when performing multiple workflows (**Supplementary Figure 1J**).

\newpage

```{r, message = FALSE}
make_speed_plots <- 
  function(speed_plot_list) {
    num_cells_list <- 
      map_dbl(
        .x = speed_plot_list, 
        .f = ~ max(.x$data$num_cells)
      )
    breaks_list <- 
      map(
        .x = num_cells_list, 
        .f = ~ c(0, .x / 2, .x)
      )
    speed_plot_list <- 
      map(
        .x = speed_plot_list, 
        .f = ~ .x + 
          scale_fill_manual(values = charizard, labels = engine_labels) + 
          scale_color_manual(values = charizard, labels = engine_labels) + 
          scale_x_continuous(
            breaks = scales::breaks_pretty(n = 3), 
            labels = scales::label_scientific(digits = 1)
          )
      )
    result <- 
      speed_plot_list |> 
      wrap_plots() + 
      plot_layout(ncol = 3, guides = "collect") + 
      plot_annotation(tag_levels = "A") + 
      theme(plot.tag = element_text(face = "bold"))
    return(result)
  }
```

```{r, fig.width = 10, fig.height = 8, warning = FALSE, message = FALSE}
all_speed_names <- 
  ls()[str_detect(string = ls(), pattern = "speed_plots(_csv)?$")] |> 
  discard(.p = ~ .x == "make_speed_plots") |> 
  factor(levels = 
           c("io_speed_plots", "io_speed_plots_csv", "preprocess_speed_plots",
             "downsample_speed_plots", "flowsom_speed_plots", "extract_speed_plots", 
             "pca_speed_plots", "tsne_speed_plots", "umap_speed_plots")
  ) |> 
  sort() |> 
  as.character()

all_speed_plots <- 
  all_speed_names |> 
  mget(envir = globalenv()) |> 
  map(.f = ~ pluck(.x, "full_plot")) |> 
  make_speed_plots()

# print(all_speed_plots)

all_speed_plots |> 
  ggsave(
    filename = 
      here::here("benchmarking", "all_speed_plots.png"), 
    device = "png", 
    width = 10, 
    height = 8
  )


all_speed_plots |> 
  ggsave(
    filename = 
      here::here("benchmarking", "all_speed_plots.pdf"), 
    device = "pdf", 
    width = 10, 
    height = 8
  )
```

```{r, include = FALSE}
#**Supplementary Figure 1 - {tidytof}'s computational speed rivals or improves upon equivalent approaches in existing tools.** Benchmark plots indicating the elapsed time for **(A)** reading FCS files, **(B)** reading CSV files, **(C)** preprocessing single-cell data, **(D)** downsampling single-cell data, **(E)** performing FlowSOM clustering, **(F)** sample-level feature extraction, **(G)** performing principal component analysis (PCA), **(H)** performing t-stochastic neighborhood embedding (tSNE), **(I)** or performing uniform manifold approximation and projection (UMAP) embedding using {tidytof}, base R, {flowCore}, {cytofkit}, {immunoCluster}, and {Spectre}. Points represent the median runtime for each workflow from 10 independent repetitions; error bars represent the interquartile range of runtimes.
```



```{r, fig.width = 12, fig.height = 7}
# option 1
combined_speed_plots <- 
  total_time_plot + 
  scale_x_continuous(
    breaks = scales::pretty_breaks(n = 4), 
    labels = scales::label_scientific(digits = 2)
  ) + 
  all_speed_plots + 
  plot_layout(guides = "collect", widths = c(3.75, 11.25), tag_level = "keep") + 
  plot_annotation(tag_levels = "A")
#print(combined_speed_plots)

new_speed_plots <- all_speed_plots
for (i in 1:9) { 
  new_speed_plots[[i]] <- all_speed_plots[[i]] + theme(legend.position = "none")
}

# option 2
combined_speed_plots <- 
  wrap_plots(
    new_speed_plots, 
    total_time_plot + 
      scale_x_continuous(
        breaks = scales::pretty_breaks(n = 4), 
        labels = scales::label_scientific(digits = 2)
      ), 
    guides = "collect",
    widths = c(11.25, 3.75), 
    tag_level = "new"
  ) +
  plot_annotation(tag_levels = "A") + 
  theme(plot.tag = element_text(face = "bold"))

print(combined_speed_plots)


combined_speed_plots |> 
  ggsave(
    filename = 
      here::here("benchmarking", "combined_speed_plots.pdf"), 
    device = "pdf", 
    width = 12, 
    height = 6
  )
```
**Supplementary Figure 1 - {tidytof}'s computational speed rivals or improves upon equivalent approaches using existing tools.** Benchmark plots indicating the elapsed time for **(A)** reading FCS files, **(B)** reading CSV files, **(C)** preprocessing single-cell data, **(D)** downsampling single-cell data, **(E)** performing FlowSOM clustering, **(F)** sample-level feature extraction, **(G)** performing principal component analysis (PCA), **(H)** performing t-stochastic neighborhood embedding (tSNE), or **(I)**  performing uniform manifold approximation and projection (UMAP) embedding using {tidytof}, base R, {flowCore}, {cytofkit}, {immunoCluster}, and {Spectre}. Panel **(J)** shows the sum of runtimes from panels A-F for each tool (because panels G-I were run on different datasets, they could not be combined with the other panels). In all panels, points represent the median runtime for each workflow from 10 independent repetitions and error bars represent the interquartile range of runtimes.


\newpage 

### Memory benchmarking

```{r}
average_increase <- 
  ((average_mem_ratio - 1) * 100) |> 
  round(2) |> 
  str_c("%")
```


Despite the increased simplicity of {tidytof}'s tidy data structures (`tof_tbl` objects) compared to {flowCore}'s native data structure (the `flowSet`), both data structures require similar amounts of system memory (**Supplementary Figure 2**). Furthermore, {tidytof} requires slightly less memory than 
{Spectre} `data.table`s and substantially less memory than {immunoCluster} `SingleCellExperiment`s and {cytofkit} `data.frame`s across all dataset sizes.

\

```{r}
memory_plot + 
  scale_x_continuous(breaks = scales::breaks_extended(n = 5), labels = scales::label_scientific())
```


**Supplementary Figure 2 - {tidytof}'s tidy data structures are memory-efficient compared to existing software tools** The system memory (in Megabytes; MB) required to store {tidytof}, {flowCore}, {cytofkit}, {immunoCluster}, and {Spectre} data structures of varying sizes. Overall, {tidytof} is competitive with {flowCore} and {Spectre} while offering substantial improvements in memory efficiency relative to {cytofkit} and {immunoCluster}.


\newpage 

### Coding burden benchmarking

In addition to its competitive computational speed relative to equivalent workflows using other cytometry data analysis software, {tidytof} reduces the coding burden of performing CyTOF data analyses significantly compared to alternative frameworks. Across workflows, {tidytof} reduces the lines of code needed to perform an analysis between 33% and 95% compared to alternative software tools (**Supplementary Figure 3**). Similarly, {tidytof} reduces the number of variable assignments needed to perform an analysis between 67% to 98% compared alternative software tools (**Supplementary Figure 4**). Because code bases with both more lines of code and more intermediate variables are more prone to human (e.g copy-and-paste) error and reduced readability, these results support {tidytof}'s marked ability to simplify high-dimensional cytometry data analysis code and increase reproducibility among data analysis pipelines.

\newpage

```{r}
make_style_plot <- function(raw, base, flowcore, cytofkit) { 
  base <- base + theme(legend.position = "none")
  flowcore <- flowcore + theme(legend.position = "none")
  cytofkit <- cytofkit + theme(legend.position = "none")
  
  result <- 
    raw +
    (base + 
       flowcore + 
       plot_layout(nrow = 1, widths = c(5.8, 4.2))
    ) + 
    cytofkit +
    plot_layout(
      guides = "collect", 
      heights = c(4, 6, 6),
      design = "
      #1111#
      222222
      333###
    "
    ) + 
    plot_annotation(tag_levels = "A") + 
    theme(plot.tag = element_text(face = "bold"))
  return(result)
}

make_style_plot <- function(raw, base, flowcore, cytofkit, immunocluster, spectre) {
  base <- base + theme(legend.position = "none")
  flowcore <- flowcore + theme(legend.position = "none")
  cytofkit <- cytofkit + theme(legend.position = "none")
  immunocluster <- immunocluster + theme(legend.position = "none")
  spectre <- spectre + theme(legend.position = "none")
  
  result <- 
    raw +
    base + 
    flowcore + 
    cytofkit +
    immunocluster + 
    spectre + 
    plot_layout(
      guides = "collect", 
      heights = c(6, 6, 6),
      design = "
      111222
      333444
      555666
    "
    ) + 
    plot_annotation(tag_levels = "A") + 
    theme(plot.tag = element_text(face = "bold"))
  return(result)
}


```


```{r lines_of_code_patchwork, fig.width = 11, fig.height = 12}
lines_patchwork <- 
  make_style_plot(
    raw = lines_plot_raw, 
    base = lines_plots$base, 
    flowcore = lines_plots$flowcore,
    cytofkit = lines_plots$cytofkit, 
    immunocluster = lines_plots$immunocluster, 
    spectre = lines_plots$spectre
  )

print(lines_patchwork)


lines_patchwork |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_patchwork.png"), 
    device = "png", 
    width = 11, 
    height = 12
  )


lines_patchwork |> 
  ggsave(
    filename = 
      here::here("benchmarking", "lines_patchwork.pdf"), 
    device = "pdf", 
    width = 11, 
    height = 12
  )
  
```


**Supplementary Figure 3 - {tidytof} reduces the number of lines of code needed for high-dimensional cytometry data analysis pipelines.** **(A)** Raw lines of code needed to perform equivalent analyses using {tidytof} and other open-source single-cell data analysis platforms. **(B-F)** The relative (%) lines of code needed to perform equivalent analyses between {tidytof} and base R (B); between {tidytof} and {flowCore} (C); between {tidytof} and {cytofkit} (D); between {tidytof} and {immunoCluster} (E); and between {tidytof} and {Spectre} (F).

\newpage 


```{r, fig.width = 11, fig.height = 12}
variables_patchwork <- 
  make_style_plot(
    raw = variables_plot_raw, 
    base = variables_plots$base, 
    flowcore = variables_plots$flowcore, 
    cytofkit = variables_plots$cytofkit, 
    immunocluster = variables_plots$immunocluster, 
    spectre = variables_plots$spectre
  )

print(variables_patchwork)

variables_patchwork |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_patchwork.png"), 
    device = "png", 
    width = 11, 
    height = 12
  )

variables_patchwork |> 
  ggsave(
    filename = 
      here::here("benchmarking", "variables_patchwork.pdf"), 
    device = "pdf", 
    width = 11, 
    height = 12
  )
  
```


**Supplementary Figure 4 - {tidytof} reduces the number of intermediate variable assignments needed for high-dimensional cytometry data analysis pipelines.** **(A)** Raw number of intermediate variable assignments needed to perform equivalent analyses using {tidytof} and other open-source single-cell data analysis platforms. **(B-F)** The relative (%) number of intermediate variable assignments needed to perform equivalent analyses between {tidytof} and base R (B); between {tidytof} and {flowCore} (C); between {tidytof} and {cytofkit} (D); between {tidytof} and {immunoCluster} (E); and between {tidytof} and {Spectre} (F).



